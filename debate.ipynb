{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMffRFRfEcYqmYFvIk6/Jii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samunderSingh12/debate_baby/blob/main/debate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Libraries\n",
        "!pip install -q gradio openai\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z_vv2AzCIC_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Import Libraries and Define Functions (with Enhanced Interaction Prompts)\n",
        "\n",
        "import gradio as gr\n",
        "import openai\n",
        "import time\n",
        "import textwrap\n",
        "\n",
        "\n",
        "DEFAULT_OPENROUTER_MODEL = \"mistralai/mistral-7b-instruct:free\"\n",
        "DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
        "DEFAULT_MAX_TOKENS = 300 # Slightly increased default\n",
        "DEFAULT_DEBATE_TURNS = 3\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "\n",
        "DEFAULT_OPENAI_SYS_PROMPT = (\n",
        "    \"You are a precise and analytical AI debater representing the OpenAI perspective. \"\n",
        "    \"Engage directly with your opponent's arguments, referencing specific points they've made throughout the debate where relevant. \"\n",
        "    \"Maintain a logical flow and build upon your previous arguments. Your goal is a constructive exchange of ideas.\"\n",
        ")\n",
        "DEFAULT_OPENROUTER_SYS_PROMPT = (\n",
        "    \"You are a creative and insightful AI debater representing the OpenRouter perspective. \"\n",
        "    \"Challenge your opponent's points thoughtfully and connect your arguments back to the core topic. \"\n",
        "    \"Feel free to refer to earlier statements in the debate to highlight consistencies or contradictions. Aim for a compelling and engaging discussion.\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_openai_client(api_key):\n",
        "    if not api_key: raise gr.Error(\"OpenAI API Key is missing!\")\n",
        "    try:\n",
        "        client = openai.OpenAI(api_key=api_key)\n",
        "        client.models.list(); return client\n",
        "    except openai.AuthenticationError: raise gr.Error(\"Invalid OpenAI API Key.\")\n",
        "    except Exception as e: raise gr.Error(f\"OpenAI Client Error: {e}\")\n",
        "\n",
        "def get_openrouter_client(api_key):\n",
        "    if not api_key: raise gr.Error(\"OpenRouter API Key is missing!\")\n",
        "    try:\n",
        "        client = openai.OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\", api_key=api_key,\n",
        "            default_headers={\"HTTP-Referer\": \"http://localhost:7860\", \"X-Title\": \"Colab AI Debate v5\"},\n",
        "        )\n",
        "        client.models.list(); return client\n",
        "    except openai.AuthenticationError: raise gr.Error(\"Invalid OpenRouter API Key.\")\n",
        "    except Exception as e: raise gr.Error(f\"OpenRouter Client Error: {e}\")\n",
        "\n",
        "\n",
        "def run_debate(topic,\n",
        "               openai_key, openrouter_key,\n",
        "               openai_model_name, openrouter_model_name,\n",
        "               openai_system_prompt, openrouter_system_prompt, # Custom prompts\n",
        "               max_tokens_input):\n",
        "\n",
        "    # --- Input Validation (Unchanged) ---\n",
        "    if not topic: raise gr.Error(\"Please provide a debate topic!\")\n",
        "    # ... (keep other validations for keys, models, prompts, tokens) ...\n",
        "    if not openai_key: raise gr.Error(\"Please provide your OpenAI API Key!\")\n",
        "    if not openrouter_key: raise gr.Error(\"Please provide your OpenRouter API Key!\")\n",
        "    if not openai_model_name: raise gr.Error(\"Please provide an OpenAI Model Name!\")\n",
        "    if not openrouter_model_name: raise gr.Error(\"Please provide an OpenRouter Model Name!\")\n",
        "    if not openai_system_prompt: raise gr.Error(\"Please provide an OpenAI System Prompt!\")\n",
        "    if not openrouter_system_prompt: raise gr.Error(\"Please provide an OpenRouter System Prompt!\")\n",
        "    try:\n",
        "        max_tokens_per_turn = int(max_tokens_input)\n",
        "        if max_tokens_per_turn <= 0: raise ValueError(\"Max tokens must be positive.\")\n",
        "    except (ValueError, TypeError):\n",
        "        raise gr.Error(\"Invalid Max Tokens value. Please use the slider or enter a positive number.\")\n",
        "\n",
        "\n",
        "    # --- Initialize Clients (Unchanged) ---\n",
        "    try:\n",
        "        yield \"Initializing Clients...\", \"\"\n",
        "        openai_client = get_openai_client(openai_key)\n",
        "        openrouter_client = get_openrouter_client(openrouter_key)\n",
        "    except gr.Error as e:\n",
        "        yield f\"Initialization Error: {e}\", \"Error\"; return\n",
        "\n",
        "    # --- Setup Debate (Unchanged logic, using inputs) ---\n",
        "    model_a_name = f\"OpenAI ({openai_model_name})\"\n",
        "    model_b_name = f\"OpenRouter ({openrouter_model_name})\"\n",
        "    model_a_client = openai_client\n",
        "    model_b_client = openrouter_client\n",
        "    model_a_model_name = openai_model_name\n",
        "    model_b_model_name = openrouter_model_name\n",
        "    model_a_system_prompt = openai_system_prompt\n",
        "    model_b_system_prompt = openrouter_system_prompt\n",
        "\n",
        "    conversation_history = [] # Stores {'role': 'user'|'assistant', 'content': ...} pairs\n",
        "\n",
        "    debate_transcript = ( # Header remains same as v4\n",
        "        f\"## Debate Topic: {topic}\\n\\n\"\n",
        "        f\"**Settings:**\\n\"\n",
        "        f\"- OpenAI Model: `{openai_model_name}`\\n\"\n",
        "        f\"- OpenRouter Model: `{openrouter_model_name}`\\n\"\n",
        "        f\"- Max Tokens: {max_tokens_per_turn}\\n\"\n",
        "        f\"- OpenAI Persona: *{textwrap.shorten(model_a_system_prompt, 100)}*\\n\"\n",
        "        f\"- OpenRouter Persona: *{textwrap.shorten(model_b_system_prompt, 100)}*\\n\\n\"\n",
        "        f\"---\\n\\n\"\n",
        "    )\n",
        "    spinner_updates = [\"Thinking.\", \"Thinking..\", \"Thinking...\"]\n",
        "\n",
        "    # --- Run Debate Turns ---\n",
        "    try:\n",
        "        current_user_instruction_text = \"\"\n",
        "\n",
        "        for turn in range(DEFAULT_DEBATE_TURNS * 2):\n",
        "            is_model_a_turn = turn % 2 == 0\n",
        "            current_model_name = model_a_name if is_model_a_turn else model_b_name\n",
        "            current_client = model_a_client if is_model_a_turn else model_b_client\n",
        "            current_model = model_a_model_name if is_model_a_turn else model_b_model_name\n",
        "            current_system_prompt = model_a_system_prompt if is_model_a_turn else model_b_system_prompt\n",
        "\n",
        "            status_message = f\"Turn {turn // 2 + 1} / {DEFAULT_DEBATE_TURNS} - {current_model_name} {spinner_updates[turn % 3]}\"\n",
        "            print(f\"--- {status_message} ---\")\n",
        "            yield debate_transcript, status_message\n",
        "\n",
        "            # --- NEW: Refined User Instruction Prompting ---\n",
        "            if turn == 0:\n",
        "                # Initial turn instruction remains simple\n",
        "                current_user_instruction_text = f\"Begin the debate by presenting your opening statement on the topic: '{topic}'.\"\n",
        "            else:\n",
        "                # Subsequent turns: Encourage considering history while responding to the last point\n",
        "                last_message_content = conversation_history[-1]['content'] # Get previous assistant response\n",
        "                # Subtle change: Frame the request to consider the history implicitly\n",
        "                current_user_instruction_text = (\n",
        "                    f\"Considering the debate history so far, present your response to the opponent's previous statement. \"\n",
        "                    f\"Opponent's statement: '{textwrap.shorten(last_message_content, width=150, placeholder='...')}'\"\n",
        "                )\n",
        "            # --- End Refined Prompting ---\n",
        "\n",
        "            # Build messages list dynamically (System + History + Current Instruction)\n",
        "            messages_for_api_call = [{\"role\": \"system\", \"content\": current_system_prompt}]\n",
        "            messages_for_api_call.extend(conversation_history) # Add history turns\n",
        "            messages_for_api_call.append({\"role\": \"user\", \"content\": current_user_instruction_text}) # Add this turn's instruction\n",
        "\n",
        "            # --- API Call (Memory concern: Check potential token count before sending if needed) ---\n",
        "            # Note: Simple check, not precise token counting. Models usually handle ~4k-8k+, up to 128k+ tokens.\n",
        "            approx_msg_count = len(messages_for_api_call)\n",
        "            if approx_msg_count > 50: # Arbitrary threshold, adjust if needed\n",
        "                 print(f\"Warning: Sending {approx_msg_count} messages, potential context length issue.\")\n",
        "\n",
        "            try:\n",
        "                print(f\"Sending {len(messages_for_api_call)} messages to {current_model_name}\")\n",
        "                response = current_client.chat.completions.create(\n",
        "                    model=current_model,\n",
        "                    messages=messages_for_api_call,\n",
        "                    max_tokens=max_tokens_per_turn,\n",
        "                    temperature=TEMPERATURE,\n",
        "                )\n",
        "                ai_response_content = response.choices[0].message.content.strip()\n",
        "\n",
        "                # Add the instruction *that led to this response* and the response itself to history\n",
        "                conversation_history.append({\"role\": \"user\", \"content\": current_user_instruction_text})\n",
        "                conversation_history.append({\"role\": \"assistant\", \"content\": ai_response_content})\n",
        "\n",
        "                debate_transcript += f\"**{current_model_name}:**\\n{ai_response_content}\\n\\n---\\n\\n\"\n",
        "                time.sleep(1.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_detail = str(e)\n",
        "                # ... (keep specific error checks: Auth, RateLimit, NotFound, ContextLength) ...\n",
        "                if \"AuthenticationError\" in error_detail: error_message_display = f\"API Auth Error ({current_model_name}). Check Key.\"\n",
        "                elif \"RateLimitError\" in error_detail: error_message_display = f\"Rate Limit Error ({current_model_name}). Wait & retry.\"\n",
        "                elif \"NotFoundError\" in error_detail and \"model\" in error_detail: error_message_display = f\"Model Not Found ({current_model}). Check Name/Access.\"\n",
        "                elif (\"BadRequestError\" in error_detail and \"context_length\" in error_detail) or \\\n",
        "                     (\"invalid_request_error\" in error_detail and \"maximum context length\" in error_detail.lower()):\n",
        "                    error_message_display = f\"Context Length Exceeded ({current_model_name}). Reduce turns/max_tokens or use a model with larger context.\"\n",
        "                else: error_message_display = f\"API Error ({current_model_name}): Check console.\"\n",
        "\n",
        "                error_log_message = f\"\\n\\n**API Error during {current_model_name}'s turn:** {e}\\nDebate halted.\"\n",
        "                print(error_log_message)\n",
        "                debate_transcript += f\"**SYSTEM:**\\n*{error_message_display}*\\n\\n---\\n\\n\"\n",
        "                yield debate_transcript, f\"Error: {current_model_name}\"\n",
        "                return\n",
        "\n",
        "        final_message = \"Debate Complete!\"\n",
        "        print(final_message)\n",
        "        yield debate_transcript, final_message\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"\\n\\n**An unexpected error occurred:** {e}\\nDebate halted.\"\n",
        "        print(error_message)\n",
        "        debate_transcript += f\"**SYSTEM:**\\n*An unexpected error occurred: {e}*\\n\\n---\\n\\n\"\n",
        "        yield debate_transcript, \"An unexpected error occurred.\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "qMG1J4m2PLJY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Create and Launch Gradio Interface\n",
        "\n",
        "# Clear any previous Gradio launches\n",
        "gr.close_all()\n",
        "\n",
        "# Define the Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 🤖 AI Debate Arena 🤺 (v5: Enhanced Interaction)\n",
        "        Set debate topic, API keys, models, max tokens, and custom system prompts (personas) for each AI.\n",
        "        The prompts now encourage deeper engagement with the debate history for a more immersive experience.\n",
        "        **Note:** Context limits can still be reached in long debates. OpenAI usage incurs costs.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # --- Left Column: Inputs & Controls (UI layout unchanged from v4) ---\n",
        "        with gr.Column(scale=1):\n",
        "            topic_input = gr.Textbox(\n",
        "                label=\"Debate Topic\", placeholder=\"e.g., Should AI have rights?\", lines=2\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Credentials\")\n",
        "            openai_key_input = gr.Textbox(label=\"OpenAI API Key\", type=\"password\", placeholder=\"sk-...\")\n",
        "            openrouter_key_input = gr.Textbox(label=\"OpenRouter API Key\", type=\"password\", placeholder=\"sk-or-...\")\n",
        "\n",
        "            gr.Markdown(\"### Model Selection\")\n",
        "            openai_model_input = gr.Textbox(label=\"OpenAI Model Name\", value=DEFAULT_OPENAI_MODEL)\n",
        "            openrouter_model_input = gr.Textbox(label=\"OpenRouter Model Name\", value=DEFAULT_OPENROUTER_MODEL)\n",
        "\n",
        "            gr.Markdown(\"### Persona / System Prompts\")\n",
        "            openai_system_prompt_input = gr.Textbox(\n",
        "                label=\"OpenAI System Prompt\",\n",
        "                placeholder=\"Define OpenAI's persona/role\",\n",
        "                value=DEFAULT_OPENAI_SYS_PROMPT, # Uses new default\n",
        "                lines=4 # Slightly more lines for longer default prompt\n",
        "            )\n",
        "            openrouter_system_prompt_input = gr.Textbox(\n",
        "                label=\"OpenRouter System Prompt\",\n",
        "                placeholder=\"Define OpenRouter's persona/role\",\n",
        "                value=DEFAULT_OPENROUTER_SYS_PROMPT, # Uses new default\n",
        "                lines=4 # Slightly more lines\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Debate Settings\")\n",
        "            max_tokens_slider = gr.Slider(\n",
        "                label=\"Max Tokens per Turn\", minimum=50, maximum=1500, # Increased max slightly\n",
        "                step=10, value=DEFAULT_MAX_TOKENS\n",
        "            )\n",
        "\n",
        "            status_output = gr.Textbox(label=\"Status\", placeholder=\"Waiting to start...\", interactive=False)\n",
        "            start_button = gr.Button(\"🚀 Start Debate\", variant=\"primary\")\n",
        "\n",
        "        # --- Right Column: Output (Unchanged) ---\n",
        "        with gr.Column(scale=2):\n",
        "            debate_output = gr.Markdown(label=\"Debate Transcript\", value=\"*Debate transcript will appear here...*\")\n",
        "\n",
        "    # --- Button Click Actions (Input list unchanged from v4) ---\n",
        "    start_button.click(\n",
        "        fn=run_debate,\n",
        "        inputs=[\n",
        "            topic_input,\n",
        "            openai_key_input, openrouter_key_input,\n",
        "            openai_model_input, openrouter_model_input,\n",
        "            openai_system_prompt_input, openrouter_system_prompt_input,\n",
        "            max_tokens_slider\n",
        "        ],\n",
        "        outputs=[debate_output, status_output]\n",
        "    )\n",
        "\n",
        "    # Examples (Unchanged)\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"Is artificial general intelligence (AGI) achievable within the next 20 years?\"],\n",
        "            [\"Should universal basic income (UBI) be implemented globally?\"],\n",
        "            [\"The impact of social media on mental health: net positive or negative?\"]\n",
        "        ],\n",
        "        inputs=[topic_input], label=\"Example Debate Topics\"\n",
        "    )\n",
        "\n",
        "# --- Launch App ---\n",
        "demo.launch(share=True, debug=False)\n",
        "\n",
        "print(\"\\n✅ Gradio app launched!\")\n",
        "print(\"👉 Click the 'Running on public URL' link above.\")\n",
        "print(\"\\n✨ New Features: Enhanced prompts aim for deeper interaction and referencing previous points.\")\n",
        "print(\"🔧 Customize personas, models, and max tokens.\")\n",
        "print(\"\\n⚠️ Context window limits are still a possibility in long debates. Monitor token usage if needed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "I2AhlMoEPTvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}